{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c3719e625422e6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0035cc95c2dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/opt/itmo-bigdata/hw3'\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64c354ee0f85e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Block #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae193dcaa730d88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Default dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a892a7b8d49fa4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:29:30.556734Z",
     "start_time": "2023-12-25T21:29:30.551744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/itmo-bigdata/hw3/kafka-notebook\n"
     ]
    }
   ],
   "source": [
    "%cd '/opt/itmo-bigdata/hw3/kafka-notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93305efdac766494",
   "metadata": {},
   "source": [
    "### Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a377e9537b4317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:29:33.134737Z",
     "start_time": "2023-12-25T21:29:33.125263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting producer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile producer.py\n",
    "import argparse\n",
    "import random\n",
    "from json import dumps\n",
    "from time import sleep\n",
    "from kafka import KafkaProducer, errors\n",
    "\n",
    "def write_data(producer, data_cnt):\n",
    "    topic = \"hw3_topic\"\n",
    "    for i in range(data_cnt):\n",
    "        device_id = random.randint(1, 10)\n",
    "        temperature = random.uniform(60, 110) + 273\n",
    "        execution_time = i * 5\n",
    "        cur_data = {\"device_id\": device_id, \"temperature\": temperature, \"execution_time\": execution_time}\n",
    "        producer.send(topic,\n",
    "                      key=dumps(device_id).encode('utf-8'),\n",
    "                      value=cur_data)\n",
    "        print(f\"Data was sent to topic [{topic}]: {cur_data}\")\n",
    "        sleep(1)\n",
    "\n",
    "def create_producer():\n",
    "    print(\"Connecting to Kafka brokers\")\n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=['kafka:9092'],\n",
    "                                 value_serializer=lambda x: dumps(x).encode('utf-8'),\n",
    "                                 acks=1)\n",
    "        print(\"Connected to Kafka\")\n",
    "        return producer\n",
    "    except errors.NoBrokersAvailable:\n",
    "        print(\"Waiting for brokers to become available\")\n",
    "        raise RuntimeError(\"Failed to connect to brokers within retry limit\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Kafka Producer\")\n",
    "    parser.add_argument(\"--message-count\", type=int, help=\"Number of messages to produce\", default=10)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    producer = create_producer()\n",
    "    write_data(producer, args.message_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a816c00dbf659c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:30:06.143676Z",
     "start_time": "2023-12-25T21:29:34.337869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Kafka brokers\r\n",
      "Connected to Kafka\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 5, 'temperature': 378.8648169030971, 'execution_time': 0}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 368.30924691389487, 'execution_time': 5}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 2, 'temperature': 367.55649140998236, 'execution_time': 10}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 8, 'temperature': 364.72706033537463, 'execution_time': 15}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 365.4685412219744, 'execution_time': 20}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 4, 'temperature': 355.23828862098406, 'execution_time': 25}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 2, 'temperature': 371.29977934694836, 'execution_time': 30}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 3, 'temperature': 369.2200390614406, 'execution_time': 35}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 7, 'temperature': 347.34340211741454, 'execution_time': 40}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 377.02934322006263, 'execution_time': 45}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 370.1921155242377, 'execution_time': 50}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 10, 'temperature': 380.8529872453549, 'execution_time': 55}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 8, 'temperature': 378.66128993108356, 'execution_time': 60}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 5, 'temperature': 381.06681311497505, 'execution_time': 65}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 7, 'temperature': 374.9846545086825, 'execution_time': 70}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 5, 'temperature': 356.84400674263827, 'execution_time': 75}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 3, 'temperature': 375.30019520262204, 'execution_time': 80}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 2, 'temperature': 349.2667321489395, 'execution_time': 85}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 5, 'temperature': 364.65789477728674, 'execution_time': 90}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 1, 'temperature': 374.42748023153086, 'execution_time': 95}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 4, 'temperature': 370.95929147668323, 'execution_time': 100}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 1, 'temperature': 375.5881243721859, 'execution_time': 105}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 7, 'temperature': 357.1490970558234, 'execution_time': 110}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 351.6161854292768, 'execution_time': 115}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 6, 'temperature': 337.0633442938946, 'execution_time': 120}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 1, 'temperature': 351.84920666394805, 'execution_time': 125}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 10, 'temperature': 375.68143177908814, 'execution_time': 130}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 1, 'temperature': 366.37713160785734, 'execution_time': 135}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 5, 'temperature': 365.2302476524959, 'execution_time': 140}\r\n",
      "Data was sent to topic [hw3_topic]: {'device_id': 3, 'temperature': 376.1214066492818, 'execution_time': 145}\r\n"
     ]
    }
   ],
   "source": [
    "!python3 producer.py --message-count 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2f183fd8000f0",
   "metadata": {},
   "source": [
    "\n",
    "### Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed70714f3e6faee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:30:14.698906Z",
     "start_time": "2023-12-25T21:30:14.697681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting consumer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile consumer.py\n",
    "import argparse\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "def create_consumer(max_messages=None):\n",
    "    print(\"Connecting to Kafka brokers\")\n",
    "    consumer = KafkaConsumer(\"hw3_topic\",\n",
    "                             group_id=\"itmo_hw3_group\",\n",
    "                             bootstrap_servers='kafka:9092',\n",
    "                             auto_offset_reset='earliest',\n",
    "                             enable_auto_commit=True)\n",
    "\n",
    "    message_count = 0\n",
    "    for message in consumer:\n",
    "        print(message)\n",
    "\n",
    "        message_count += 1\n",
    "\n",
    "        if max_messages and message_count >= max_messages:\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Kafka Consumer Example\")\n",
    "    parser.add_argument(\"--message-limit\", type=int, help=\"Maximum number of messages to consume\", default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    create_consumer(max_messages=args.message_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f5f47e92517c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:30:19.117333Z",
     "start_time": "2023-12-25T21:30:16.841890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Kafka brokers\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=0, timestamp=1703539775770, timestamp_type=0, key=b'5', value=b'{\"device_id\": 5, \"temperature\": 378.8648169030971, \"execution_time\": 0}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=71, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=1, timestamp=1703539776772, timestamp_type=0, key=b'6', value=b'{\"device_id\": 6, \"temperature\": 368.30924691389487, \"execution_time\": 5}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=72, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=2, timestamp=1703539777774, timestamp_type=0, key=b'2', value=b'{\"device_id\": 2, \"temperature\": 367.55649140998236, \"execution_time\": 10}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=3, timestamp=1703539778778, timestamp_type=0, key=b'8', value=b'{\"device_id\": 8, \"temperature\": 364.72706033537463, \"execution_time\": 15}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=4, timestamp=1703539779783, timestamp_type=0, key=b'6', value=b'{\"device_id\": 6, \"temperature\": 365.4685412219744, \"execution_time\": 20}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=72, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=5, timestamp=1703539780786, timestamp_type=0, key=b'4', value=b'{\"device_id\": 4, \"temperature\": 355.23828862098406, \"execution_time\": 25}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=6, timestamp=1703539781789, timestamp_type=0, key=b'2', value=b'{\"device_id\": 2, \"temperature\": 371.29977934694836, \"execution_time\": 30}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=7, timestamp=1703539782795, timestamp_type=0, key=b'3', value=b'{\"device_id\": 3, \"temperature\": 369.2200390614406, \"execution_time\": 35}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=72, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=8, timestamp=1703539783799, timestamp_type=0, key=b'7', value=b'{\"device_id\": 7, \"temperature\": 347.34340211741454, \"execution_time\": 40}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n",
      "ConsumerRecord(topic='hw3_topic', partition=0, offset=9, timestamp=1703539784803, timestamp_type=0, key=b'6', value=b'{\"device_id\": 6, \"temperature\": 377.02934322006263, \"execution_time\": 45}', headers=[], checksum=None, serialized_key_size=1, serialized_value_size=73, serialized_header_size=-1)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 consumer.py --message-limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82317c26e857ae7",
   "metadata": {},
   "source": [
    "### Apache Flink job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3664c23a745373d4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T21:30:24.235193Z",
     "start_time": "2023-12-25T21:30:23.596764Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p tmp/checkpoints/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7ba506c35c075b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T21:30:28.180019Z",
     "start_time": "2023-12-25T21:30:28.173558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing flink_job1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile flink_job1.py\n",
    "from argparse import ArgumentParser\n",
    "from pyflink.common import SimpleStringSchema\n",
    "from pyflink.common.typeinfo import Types, RowTypeInfo\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import DeliveryGuarantee\n",
    "from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer, KafkaSink, KafkaRecordSerializationSchema\n",
    "from pyflink.datastream.formats.json import JsonRowDeserializationSchema\n",
    "from pyflink.datastream.functions import MapFunction\n",
    "from pyflink.datastream.checkpoint_config import CheckpointingMode\n",
    "\n",
    "\n",
    "def python_data_stream_example(checkpoint_dir):\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "\n",
    "    env.enable_checkpointing(10000)  # checkpoint every 10000 ms\n",
    "    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)\n",
    "    env.get_checkpoint_config().set_min_pause_between_checkpoints(500)\n",
    "    env.get_checkpoint_config().set_max_concurrent_checkpoints(1)\n",
    "    env.get_checkpoint_config().set_checkpoint_storage_dir(checkpoint_dir)\n",
    "\n",
    "    type_info: RowTypeInfo = Types.ROW_NAMED([\"device_id\", \"temperature\", \"execution_time\"],\n",
    "                                             [Types.LONG(), Types.DOUBLE(), Types.INT()])\n",
    "\n",
    "    json_row_schema = JsonRowDeserializationSchema.builder().type_info(type_info).build()\n",
    "\n",
    "    source = KafkaSource.builder() \\\n",
    "        .set_bootstrap_servers('kafka:9092') \\\n",
    "        .set_topics('hw3_topic') \\\n",
    "        .set_group_id('pyflink-e2e-source') \\\n",
    "        .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\\n",
    "        .set_value_only_deserializer(json_row_schema) \\\n",
    "        .build()\n",
    "\n",
    "    sink = KafkaSink.builder() \\\n",
    "        .set_bootstrap_servers('kafka:9092') \\\n",
    "        .set_record_serializer(KafkaRecordSerializationSchema.builder()\n",
    "                               .set_topic('hw3_preprocessed_topic')\n",
    "                               .set_value_serialization_schema(SimpleStringSchema())\n",
    "                               .build()\n",
    "                               ) \\\n",
    "        .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\\n",
    "        .build()\n",
    "\n",
    "    ds = env.from_source(source, WatermarkStrategy.no_watermarks(), \"Kafka Source\")\n",
    "    ds.map(TemperatureFunction(), Types.STRING()) \\\n",
    "        .sink_to(sink)\n",
    "    env.execute_async(\"Devices preprocessing\")\n",
    "\n",
    "\n",
    "class TemperatureFunction(MapFunction):\n",
    "\n",
    "    def map(self, value):\n",
    "        device_id, temperature, execution_time = value\n",
    "        return str({\"device_id\": device_id, \"temperature\": temperature - 273, \"execution_time\": execution_time})\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description=\"Flink Job with Checkpointing\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, help=\"Directory for saving checkpoints\", default='file:///opt/pyflink/tmp/checkpoints/logs')\n",
    "    args = parser.parse_args()\n",
    "    python_data_stream_example(args.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa2ff733619c4f",
   "metadata": {},
   "source": [
    "Job execution:\n",
    "\n",
    "```commandline\n",
    "docker-compose exec jobmanager ./bin/flink run -py /opt/pyflink/flink_job1.py -d \n",
    "```\n",
    "Screen log result:\n",
    "![job](images/block1_job.png)\n",
    "\n",
    "Screen Apache Flink Example:\n",
    "![flink](images/block1_flink.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387850e9a15d2af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Block #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef609ba-0452-4760-b830-9b8ecb4c5c73",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%%writefile flink_job2.py\n",
    "from argparse import ArgumentParser\n",
    "from pyflink.common import SimpleStringSchema, Time\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors import DeliveryGuarantee\n",
    "from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer, KafkaSink, KafkaRecordSerializationSchema\n",
    "from pyflink.datastream.formats.json import JsonRowDeserializationSchema\n",
    "from pyflink.datastream.functions import MapFunction\n",
    "from pyflink.datastream.checkpoint_config import CheckpointingMode\n",
    "from pyflink.datastream.window import TumblingEventTimeWindows, SlidingEventTimeWindows, EventTimeSessionWindows\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.common.time import Duration\n",
    "\n",
    "\n",
    "class FormatTemperature(MapFunction):\n",
    "    def map(self, value):\n",
    "        return \"max_temp: {}\".format(value[1])\n",
    "\n",
    "\n",
    "def python_data_stream_example(checkpoint_dir, window_type):\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.set_parallelism(1)\n",
    "    env.enable_checkpointing(10000)\n",
    "    env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)\n",
    "    env.get_checkpoint_config().set_min_pause_between_checkpoints(500)\n",
    "    env.get_checkpoint_config().set_max_concurrent_checkpoints(1)\n",
    "    env.get_checkpoint_config().set_checkpoint_storage_dir(checkpoint_dir)\n",
    "\n",
    "    type_info = Types.ROW_NAMED([\"device_id\", \"temperature\", \"execution_time\"],\n",
    "                                [Types.LONG(), Types.DOUBLE(), Types.INT()])\n",
    "\n",
    "    json_row_schema = JsonRowDeserializationSchema.builder().type_info(type_info).build()\n",
    "\n",
    "    source = KafkaSource.builder() \\\n",
    "        .set_bootstrap_servers('kafka:9092') \\\n",
    "        .set_topics('hw3_topic') \\\n",
    "        .set_group_id('pyflink-e2e-source') \\\n",
    "        .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\\n",
    "        .set_value_only_deserializer(json_row_schema) \\\n",
    "        .build()\n",
    "\n",
    "    sink = KafkaSink.builder() \\\n",
    "        .set_bootstrap_servers('kafka:9092') \\\n",
    "        .set_record_serializer(KafkaRecordSerializationSchema.builder()\n",
    "                               .set_topic('hw3_preprocessed_topic')\n",
    "                               .set_value_serialization_schema(SimpleStringSchema())\n",
    "                               .build()\n",
    "                               ) \\\n",
    "        .set_delivery_guarantee(DeliveryGuarantee.AT_LEAST_ONCE) \\\n",
    "        .build()\n",
    "\n",
    "    watermark_strategy = WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(5)) \\\n",
    "        .with_timestamp_assigner(lambda event, timestamp: event[2])\n",
    "\n",
    "    ds = env.from_source(source, watermark_strategy, \"Kafka Source\")\n",
    "\n",
    "    if window_type == \"tumbling\":\n",
    "        ds = ds.key_by(lambda value: value[0]) \\\n",
    "            .window(TumblingEventTimeWindows.of(Time.seconds(15))) \\\n",
    "            .reduce(lambda a, b: a if a[1] > b[1] else b) \\\n",
    "            .map(FormatTemperature(), output_type=Types.STRING())\n",
    "    elif window_type == \"sliding\":\n",
    "        ds = ds.key_by(lambda value: value[0]) \\\n",
    "            .window(SlidingEventTimeWindows.of(Time.seconds(15), Time.seconds(5))) \\\n",
    "            .reduce(lambda a, b: a if a[1] > b[1] else b) \\\n",
    "            .map(FormatTemperature(), output_type=Types.STRING())\n",
    "    elif window_type == \"session\":\n",
    "        ds = ds.key_by(lambda value: value[0]) \\\n",
    "            .window(EventTimeSessionWindows.with_gap(Time.seconds(10))) \\\n",
    "            .reduce(lambda a, b: a if a[1] > b[1] else b) \\\n",
    "            .map(FormatTemperature(), output_type=Types.STRING())\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported window type: {}\".format(window_type))\n",
    "\n",
    "    ds.sink_to(sink)\n",
    "    env.execute_async(\"Devices preprocessing\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser(description=\"Flink Job with Different Window Types\")\n",
    "    parser.add_argument(\"--checkpoint_dir\", type=str, help=\"Directory for saving checkpoints\",\n",
    "                        default='file:///opt/pyflink/tmp/checkpoints/logs')\n",
    "    parser.add_argument(\"--window_type\", type=str, help=\"Type of window to apply (tumbling, sliding, session)\",\n",
    "                        default='tumbling')\n",
    "    args = parser.parse_args()\n",
    "    print(args.window_type)\n",
    "    python_data_stream_example(args.checkpoint_dir, args.window_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39da6fb282f0eb0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Job executions:\n",
    "```commandline\n",
    "docker-compose exec jobmanager ./bin/flink run -py /opt/pyflink/flink_job2.py --window_type 'tumbling' -d \n",
    "```\n",
    "```commandline\n",
    "docker-compose exec jobmanager ./bin/flink run -py /opt/pyflink/flink_job2.py --window_type 'sliding' -d \n",
    "```\n",
    "```commandline\n",
    "docker-compose exec jobmanager ./bin/flink run -py /opt/pyflink/flink_job2.py --window_type 'session' -d \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eca878-0015-4299-a649-1a5e21356ce4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Block #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52d2421251e79ef",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-25T21:39:59.434908Z",
     "start_time": "2023-12-25T21:39:59.433541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing backoff_consumer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backoff_consumer.py\n",
    "import time\n",
    "from functools import wraps\n",
    "import argparse\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "def backoff(tries, sleep):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            ntries = tries\n",
    "            while ntries > 1:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}, waiting {sleep} seconds before retry...\")\n",
    "                    time.sleep(sleep)\n",
    "                    ntries -= 1\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@backoff(tries=10, sleep=10)\n",
    "def connect_kafka_consumer():\n",
    "    try:\n",
    "        consumer = KafkaConsumer(\"hw3_topic\",\n",
    "                                 group_id=\"itmo_hw3_group\",\n",
    "                                 bootstrap_servers='kafka:9092',\n",
    "                                 auto_offset_reset='earliest',\n",
    "                                 enable_auto_commit=True)\n",
    "        print(\"Connected to Kafka\")\n",
    "        return consumer\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Kafka: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def message_handler(value):\n",
    "    print(value)\n",
    "\n",
    "\n",
    "def create_consumer(max_messages=None):\n",
    "    consumer = connect_kafka_consumer()\n",
    "\n",
    "    print(\"Consumer setup complete. Listening for messages...\")\n",
    "    message_count = 0\n",
    "    for message in consumer:\n",
    "        try:\n",
    "            message_handler(message.value)\n",
    "            message_count += 1\n",
    "            if max_messages and message_count >= max_messages:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"Kafka Consumer with Backoff for Connection\")\n",
    "    parser.add_argument(\"--message-limit\", type=int, help=\"Maximum number of messages to consume\", default=None)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    create_consumer(max_messages=args.message_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77780acb9eac72ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Example of backoff working, at the beginning docker with kafka is down, then after 30 seconds it is up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7155548eb26e2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-25T20:56:36.983885Z",
     "start_time": "2023-12-25T20:56:05.521309Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to Kafka: NoBrokersAvailable\r\n",
      "Error: NoBrokersAvailable, waiting 10 seconds before retry...\r\n",
      "Failed to connect to Kafka: NoBrokersAvailable\r\n",
      "Error: NoBrokersAvailable, waiting 10 seconds before retry...\r\n",
      "Failed to connect to Kafka: NoBrokersAvailable\r\n",
      "Error: NoBrokersAvailable, waiting 10 seconds before retry...\r\n",
      "Connected to Kafka\r\n",
      "Consumer setup complete. Listening for messages...\r\n",
      "b'{\"device_id\": 4, \"temperature\": 367.60424483866046, \"execution_time\": 0}'\r\n",
      "b'{\"device_id\": 2, \"temperature\": 364.71385965918205, \"execution_time\": 5}'\r\n",
      "b'{\"device_id\": 1, \"temperature\": 382.3306217475197, \"execution_time\": 10}'\r\n",
      "b'{\"device_id\": 9, \"temperature\": 351.5943844046902, \"execution_time\": 15}'\r\n",
      "b'{\"device_id\": 3, \"temperature\": 369.785697708518, \"execution_time\": 20}'\r\n",
      "b'{\"device_id\": 1, \"temperature\": 335.2740750198138, \"execution_time\": 25}'\r\n",
      "b'{\"device_id\": 4, \"temperature\": 382.31956139997146, \"execution_time\": 30}'\r\n",
      "b'{\"device_id\": 6, \"temperature\": 339.8556579285353, \"execution_time\": 35}'\r\n",
      "b'{\"device_id\": 1, \"temperature\": 346.7992573028407, \"execution_time\": 40}'\r\n",
      "b'{\"device_id\": 9, \"temperature\": 372.2972885690783, \"execution_time\": 45}'\r\n"
     ]
    }
   ],
   "source": [
    "!python3 backoff_consumer.py --message-limit 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
